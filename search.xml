<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[内核常见异常信息说明]]></title>
    <url>%2F2023%2F07%2F23%2F%E5%86%85%E6%A0%B8%E5%B8%B8%E8%A7%81%E5%BC%82%E5%B8%B8%E4%BF%A1%E6%81%AF%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[本文主要介绍内核常见异常以及定位方法。 内核死循环监控示例： 1BUG: soft lockup - CPU#0 stuck for 9s![stpd:233] 解释： soft lockup： 关键词，表明监测到死循环 CPU#0： this cpu，监测到死循环所在的cpu核 9s：监测到死循环的时长，一般比设置的死循环监控门限大1 [stpd:233]：监测到死循环发生时的任务名和任务id 注意：任务名和任务id说明的只是在该任务的上下文，不一定是当前任务发生了死循环，中断或者软中断过多，处理时间过长也会导致 定位思路： 1、死循环：进程或线程可能陷入了一个无限的循环中，导致无法退出。 2、资源竞争：可能存在资源竞争的情况，导致CPU#0 无法顺利释放并处理其他任务。 3、锁问题：可能存在锁的问题，如死锁或无法正确释放锁。 4、大量的中断：导致CPU#0 被卡住并无法处理其他任务 通过dmesg信息查看调用栈信息分析当前流程是否可能发生死循环。或者当前环境是否会导致大量的中断和软中断处理。 核间监测每隔1s，core(n)会对core(n+1)的软中断处理个数和中断处理个数与上一秒进行比较，若中断（软中断）个数不发生变化的时间超过门限值，会触发狗叫。 注：核间检测是否配置，通过查看/porc/cmdline中是否有ccd_threshold=28参数。28表示的是门限值，单位为s,可修改 示例：1234cpu 0 [hard cpu 0] disable softirq long times.0:NMI[#1]:...Call Trace: 解释： cpu 0：0核上被检测到，关中断/软中断时间过长 softirq：表示关软中断时间过长，若为irq，则关中断时间过长 0:NMI[#1]：0核上触发的狗叫异常 Call Trace：处理狗叫异常时会打印调用栈 CPU 0被禁用了软件中断（softirq）较长的一段时间。硬件中断（hardirq）通常优先于软件中断进行处理，但如果软件中断需要处理的任务太耗时，就会导致CPU 0无法及时进行硬件中断的处理。 定位思路： 1、通过查看Call Trace中的函数调用栈，找到导致CPU 0长时间禁用软件中断的函数。 2、通过/proc/softirqs和/proc/interrupts两个文件，查看软中断和硬中断的统计数据。可以提供有关中断处理程序正在处理以及哪些软件中断正在被禁用。 3、借助中断检测工具来查看谁最后关闭了中断或软中断。 死锁内核中的某个线程或进程持有一个锁并等待该锁被释放，而其他进程或线程无法获取该锁而无法继续执行。这种情况下，所有进程或线程都会停止响应，系统会陷入死循环或死锁状态。示例：123456BUG: spinlock lockup on CPU#1, swapper/0, 83bb3fc0lock:83bb3fc, lock owner: swapper/0, lock owner_cpu: CPU#3Call Trace:...owner&#x27;s Stack:Call Trace: 解释：swapper/0：获取锁的任务名和pid83bb3fc0：锁所在的地址，标识了一个锁lock owner：当前锁的持有者lock owner_cpu：当前持有者所在的cpuCall Trace：尝试获取锁的任务的调用栈owner’s Stack：持有锁的任务栈Call Trace：锁持有者的调用栈 定位思路：1、分析Call Trace和owner’s Stack调用栈2、在CPU3上，通过bta打印出的获取者、持有者的信息以及调用栈信息，分析相应流程是否存在死锁的情况。3、锁所在的内存被踩或访问野指针也可能会导致死锁。 原子上下文调用可能睡眠的函数示例：1234567891011BUG: sleeping function called from invalid context at down_read:21in atomic():0,irqs_disabled():1INFO:lockdep is truned off.irq event stamp: 0hardirqs last enabled at (0): [&lt;0000000000000000&gt;] 0x0hardirqs last disabled at (0): [&lt;ffffffff80245404&gt;] copy_process+0x2d4/0x12e0softirqs last enabled at (0): [&lt;ffffffff80245404&gt;] copy_process+0x2d4/0x12e0softirqs last disabled at (0): [&lt;0000000000000000&gt;] 0x0Call Trace:.....might_sleep: 内核中有的会睡眠的函数会在入口处用might_sleep检测当前上下文是否位于原子上下文 解释：down_read:21：调用might_sleep所在的文件及行号in_atomic()：1表示在原子上下文irqs_disabled()：1表示在中断上下文hardirqs last enabled at (0)：最后一次开中断的位置hardirqs last disabled at (0)：最后一次关中断的位置softirqs last enabled at (0)：最后一次开软中断的位置softirqs last disabled at (0)：最后一次关软中断的位置 定位思路：1、中断上下文：排查中断处理流程中调用可能睡眠函数2、原子上下文：排查流程中是否在获取锁后调用可能睡眠函数 注意：当前linux中允许睡眠的锁是信号量(semaphore)、互斥锁（mutex）、条件变量（condition variable） 原子上下文进行任务切换在锁住某个内核资源时，尝试调用会导致进程睡眠的操作，从而违反了内核的同步约束（如不允许睡眠锁等）引起的。示例：12BUG: scheduling while atomic: swapper/0/0x00000100Call Trace: 定位思路： 根据调用栈排查代码；尝试开启内核debugging级别的错误信息，例如Enable CONFIG_DEBUG_ATOMIC_SLEEP 打印信息，帮忙检查问题。 内存耗尽示例：12345lldpd invoked oom-killer: gfp_mask=0xd0, order=0, oomkilladj=0Call Trace:[&lt;80123a28&gt;]dump_stack+0x8/0x40[&lt;801a2564&gt;]oom_kill_process+0xd4/0x280[&lt;801a2bc4&gt;]out_of_memory+0xf4/0x180 解释：oom-killer：内存耗尽杀手（Out Of-Memory Killer）gfp_mask=0xd0：表示该进程申请内存时，使用了GFP_COMP和GFP_ZERO标志order=0：表示该进程申请的是一块大小为4KB的物理内存页dump_stack：记录内核堆栈跟踪信息oom_kill_process：系统通过调用oom_kill_process函数杀死进程lldpd，以释放内存资源out_of_memory：表示系统因为内存不足，而调用了out_of_memory函数来释放被占用的内存。 定位思路：1、在进入OOM异常前肯定打印了内存告警信息，分析内存告警中用户态内存使用和内核态各内存池分配情况，找出异常情况。2、内存门限告警信息若没有打印出，去flash或cf卡上找diag日志。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>LINUX</tag>
        <tag>OS</tag>
        <tag>内核</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHY管理接口MDIO]]></title>
    <url>%2F2021%2F05%2F20%2FPHY%E7%AE%A1%E7%90%86%E6%8E%A5%E5%8F%A3MDIO%2F</url>
    <content type="text"><![CDATA[本文主要总结MDIO接口相关知识。 MII接口MII（Media Independent Interface）(介质无关接口)或称为媒体独立接口，MII接口是MAC与PHY连接的标准接口。它是IEEE802.3的以太网行业标准。MII接口提供了MAC与PHY之间、PHY与STA（Station Management）之间的互联技术。“媒体独立”表明在不对MAC硬件重新设计或替换的情况下，任何类型的PHY设备都可以正常工作。它是一种用于将不同的PHY与相同的网络控制器（MAC）相连接的通用总线。 STA(Station management entity)：管理实体，一般为MAC或CPU，通过SMI（Serial Manage Interface）对PHY的行为、状态进行管理和控制，而具体管理和控制动作是通过读写PHY内部的寄存器实现的。 MII和MDIO关系MII接口包括一个数据接口和一个MAC和PHY之间的管理接口（MDIO接口）。数据接口包括分别用于发送器和接收器的两条独立信道，每条信道都有自己的数据时钟和控制信号。管理接口是个双信号接口：一个是时钟信号，一个是数据信号。通过管理接口，上层能监视和控制PHY。其中时钟信号就是MDC，它是由MAC输出，是非周期性的，即不要求提供固定频率的时钟，对于PHY芯片则作为输入，在上升沿触发MDIO的读写。MDC的时钟频率可以是DC-2.5MHz，即最小的时钟周期为400ns。数据信号就是MDIO，它是一根双向的数据线，MAC作为master，PHY作为slave。在写PHY寄存器的时候，由MAC驱动MDIO向PHY写入数据；在读PHY寄存器时，前半段由MAC驱动发送寄存器地址，后半段由PHY驱动回复寄存器的值。以10M，100M端口为例，MII和MDIO结构图如下： IEEE802.3在两个章节定义了PHY的管理接口： 1、22.2.4 Management functions2、45. Management Data Input/Output (MDIO) Interface Clause22和Clause 45Clause22定义GE及以下速率（10/100/1000M）PHY的管理接口；Clause45定义10G及以上速率PHY的管理接口；STA通过管理接口对PHY寄存器进行读写操作。 Clause 22帧Clause 22帧格式： 1）PREPRE即preamble。MAC访问PHY寄存器之前，连续发送32个”1“，用于和PHY进行同步。 2）STST即Start of frame，是2个固定的比特0和1。 3）OPOP即Operation Code。读操作的操作码是10，写操作的操作码是01。 4）PHYADPHYAD即PHY address，表示5个bit位的PHY地址。PHY地址是由硬件连接决定的。硬件设计时，把PHY芯片的引脚电平上拉或者下拉，就可以设定PHY的地址。首先发送接收高位地址。 5）REGADREGAD即Register Address，表示PHY内部的寄存器地址。MII管理接口中，寄存器地址是5位的。首先发送接收高位地址。 6）TATA即Turn Around。寄存器地址字段和管理帧的数据字段之间有2bit的转换间隙，以避免读操作时冲突。读操作的TA域从高阻态变为0，写操作的TA域则是从1变为0。如下图所示： 高阻态是一个数字电路里常见的术语，指的是电路的一种输出状态，既不是高电平也不是低电平，如果高阻态再输入下一级电路的话，对下级电路无任何影响，和没接一样，如果用万用表测的话有可能是高电平也有可能是低电平，随它后面接的东西定的。 7）DATA数据域为16bit，和PHY寄存器的宽度一致。读操作中，DATA域是从PHY层读到的数据；写操作中，DATA域是要写入PHY寄存器的值。 Clause 22限制由上可知，Clause 22只有5bit的寄存器空间和PHY地址空间，这样导致能访问的PHY寄存器只有32个。千兆以下的PHY比较简单，只有32个寄存器，MAC通过Clause 22管理接口访问这些PHY寄存器已经足够，但是10G以上的PHY比较复杂，PHY层被划分为多个子层，PCS子层、PMA子层、PMD子层，Clause 22很难满足要求。 Clause 22扩展前期，对于PHY上述限制，采用的一个比较通用的做法是单独设立一个PAGE寄存器，然后将寄存器映射到不同的PAGE上，每个PAGE是32Word空间。以88E1111为例，我们需要首先在Page寄存器22写值，选择对应的Page，然后再访问该Page上的寄存器。虽然这个方案暂时解决了寄存器空间不够的问题，但是由于IEEE没有对应的规范，各个厂商实现的都不一样，驱动需要根据不同的PHY芯片写不同的接口代码。88E1111寄存器如下图： Clause 45终于在2000年的时候IEEE发布了IEEE 802.3 Clause 45规范来解决这个问题。 Clause 45帧格式： Clause45 管理接口硬件上和Clause 22管理接口一致，但是管理帧结构发生了变化。通过MDIO接口访问PHY寄存器时，需要知道端口地址、MMD设备地址和寄存器地址。和Clause 22管理接口相比，Clause 45接口的读和写操作变复杂了，操作一个寄存器需要完成两步，首先要传送寄存器地址信息，然后才能读或者写对应的寄存器。和Clause 22管理接口的管理帧相比，Clause 45接口的管理帧的主要差异如下：1）STST即Start of frame。Clause 22管理接口的管理帧的ST域是01，Clause 45接口管理帧中的ST域的值是00。 2）PRTADPRTAD即Port Address。PRTAD和Clause 22管理接口的管理帧的PHY address一致。 3）DEVADDEVED是Device Address，该域是MMD设备的5位ID。这个域对应Clause 22管理接口的管理帧的寄存器地址域。 4）ADDRESS/DATAMDIO接口的地址操作中，这个域用来传送需要访问的寄存器的16位地址。Clause 22管理接口中，寄存器地址只有5位；Clause 45接口中，寄存器地址变为16位。Clause 45接口的读写操作中，这个域是读出或者待写入的数据。Clause 45接口，单个STA通过单个MDIO接口最多可以访问32个PHY，每个PHY最多支持32个MMD，每个MMD最多可支持65536个寄存器。Clause 45结构： MMD(MDIO Manageable Device):MDIO管理设备，通过IEEE802.3标准Clause 45规定的MDIO接口进行管理。在Clause 45接口的定义中，PHY层被划分为多个MMD设备，每个MMD内部的寄存器的地址分配和Clause 22管理接口中的地址类似，不过寄存器的数量多出很多。IEEE 802.3标准规定的MMD设备地址： 读写时序Clause 22读时序： 1、MDIO master 驱动输出前导符，前导符为高电平1。连续发送32个“1”，用于和PHY进行同步。2、发送2bit的固定的启动开始标志，01b3、发送2bit的操作码，读是10b4、发送5bit的Phy addr，通过该地址来识别对应的PHY芯片5、发送5bit的寄存器地址，寻址范围是32 Word6、对于读时序的前半部分由MDIO master侧驱动 ，通过一个高阻态bit time切换到slave驱动，slave驱动后驱动一个bit的07、slave 输出寻址的寄存器和16bit的值8、总线进入高阻态，读时序结束 写时序： 1、MDIO master 驱动输出前导符，前导符为高电平1。连续发生32个“1”，用于和PHY进行同步。2、发送2bit的固定的启动开始标志，01b3、发送2bit的操作码，写是01b4、发送5bit的Phy addr，通过该地址来识别对应的PHY芯片5、发送5bit的寄存器地址，寻址范围是32 Word6、发送2bit固定的周转码，为10b7、输出要写入寄存器的16bit值8、总线进入高阻态，写时序结束 Clause 45设置地址时序： 1、MDIO master驱动输出前导符，前导符为高电平1。连续发送32个“1”，用于和PHY进行同步2、发送2bit的固定的启动开始标志，00b3、发送2bit的操作码，写地址是00b4、发送5bit的Phy addr，通过该地址来识别对应的PHY芯片5、发送5bit的设备地址，寻址范围是32 Word。（可以理解为PHY的二级地址）6、发送2bit固定的周转码，为10b7、发送要写入的16bit的地址值8、总线进入高阻态，写地址时序结束 写时序： 1、MDIO master驱动输出前导符，前导符为高电平1。连续发生32个“1”，用于和PHY进行同步2、发送2bit的固定的启动开始标志，00b3、发送2bit的操作码，写操作是01b4、发送5bit的Phy addr，通过该地址来识别对应的PHY芯片5、发送5bit的设备地址，寻址范围是32 Word。（可以理解为PHY的二级地址）6、发送2bit固定的周转码，为10b7、发送要写入的16bit的地址值8、总线进入高阻态，写时序结束 读时序： 1、MDIO master驱动输出前导符，前导符为高电平1。连续发送32个“1”，用于和PHY进行同步2、发送2bit的固定的启动开始标志，00b3、发送2bit的操作码，读操作是11b4、发送5bit的Phy addr，通过该地址来识别对应的PHY芯片5、发送5bit的设备地址，寻址范围是32 Word。（可以理解为PHY的二级地址）6、发送2bit固定的周转码，为Z0b7、slave输出16bit的寄存器值8、总线进入高阻态，读时序结束 增量读时序： 1、MDIO master驱动输出前导符，前导符为高电平1。连续发送32个“1”，用于和PHY进行同步2、发送2bit的固定的启动开始标志，00b3、发送2bit的操作码，对于增量读操作是10b4、发送5bit的Phy addr，通过该地址来识别对应的PHY芯片5、发送5bit的设备地址，寻址范围是32 Word。（可以理解为PHY的二级地址）6、发送2bit固定的周转码，为Z0b7、slave输出16bit的寄存器值（每次都较上一次多偏移一单位地址）8、总线进入高阻态，增量读时序结束 增量读：就是连续读出一段空间的地址，这时候先通过设置地址的时序写寄存器的地址，再依次重复调用增量读命令去读寄存器，每次在接收到增量读地址帧并完成读操作后，MMD将地址寄存器递增1。直到读完该空间最后一个寄存器。 总结 1、Clause 45寄存器地址从5bit扩展到16bit，既可以访问到65536个寄存器2、Clause 45引入了MMD概念，每个PHY最多可支持32个MMD3、Clause 22只需要1帧就可以完成读写，而Clause 45需要2帧（第一次发送地址协议帧指定寄存器，第二次发送帧去读写数据）才可以完成读写]]></content>
      <categories>
        <category>以太网</category>
      </categories>
      <tags>
        <tag>以太网</tag>
        <tag>嵌入式</tag>
        <tag>IEEE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2021%2F05%2F20%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程睡眠]]></title>
    <url>%2F2021%2F05%2F20%2F%E8%BF%9B%E7%A8%8B%E7%9D%A1%E7%9C%A0%2F</url>
    <content type="text"><![CDATA[本文主要总结引起进程睡眠的相关知识。 进程睡眠简介进程睡眠的含义为：当前进程停止运行，系统将进行任务调度使另外一个任务运行。当唤醒睡眠的条件出现时，该进程将被唤醒，等待任务调度时恢复执行。睡眠函数的核心在于触发任务调度，即调用schedule()函数，使系统当前运行的进程发生切换。 引起睡眠的操作 哪些流程禁止调用睡眠函数上下文是什么上下文是从英文context翻译过来，指的是一种环境。相对于进程而言，就是进程执行时的环境；具体来说就是各个变量和数据，包括所有的寄存器变量、进程打开的文件、内存信息等。那为什么会有上下文呢？在介绍这个话题前，我们先了解一下下面几个知识。 内核空间和用户空间我们知道现在操作系统都是采用虚拟存储器，那么对32位操作系统而言，它的寻址空间（虚拟内存空间）为4G（2的32次方）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核，保证内核的安全，操作系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对Linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。每个进程可以通过系统调用进入内核，因此，Linux内核由系统内的所有进程共享。于是，从具体进程的角度来看，每个进程可以拥有4G字节的虚拟空间。空间分配如下图所示： 有了用户空间和内核空间，整个Linux内部结构可以分为三个部分，从最底层到最上层依次是：硬件—&gt;内核空间—&gt;用户空间。如下图所示： 特权级Intel X86架构的CPU一共有0~3四个特权级，0级最高，3级最低，ARM架构也有不同的特权级，硬件上在执行每条指令时都会对指令所具有的特权级做相应的检查。硬件已经提供了一套特权级使用的相关机制，软件自然要好好利用，这属于操作系统要做的事情，对于UNIX/LINUX来说，只使用了0级特权级别和3级特权级别，即最高最低特权级。也就是说在UNIX/LINUX系统中，一条工作在0级特权级的指令具有了CPU能提供的最高权力，而一条工作在3级特权的指令具有CPU提供的最低或者说最基本权力。以上是从CPU执行指令角度理解特权，其实虚拟地址到物理地址映射有mmu硬件实现，即分页机制是硬件对分页的支持，进程中有页表数据结构指向用户空间和内核空间，使用户态和内核态访问内存空间不同。 现在我们从特权级的调度来理解用户态和内核态就比较好理解了，当程序运行在3级特权级上时，就可以称之为运行在用户态，因为这是最低特权级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在内核态；反之，当程序运行在0级特权级上时，就可以称之为运行在内核态。虽然用户态下和内核态下工作的程序有很多差别，但最重要的差别就在于特权级的不同，即权力的不同。运行在用户态的程序不能访问操作系统内核数据结构和程序。当我们在系统中执行一个程序时，大部分时间是运行在用户态下的。在其需要操作系统帮助完成某些它没有权力和能力完成的工作时就会切换到内核态。Linux进程的4GB地址空间，3G-4G部分大家是共享的，是内核态的地址空间，这里存放着整个内核的代码和所有的内核模块，以及内核所维护的数据。用户运行一个程序，该程序所创建的进程开始是运行在用户态的，如果要执行文件操作，网络数据发送等操作，必须通过write，send等系统调用，这些系统调用会调用内核中的代码来完成操作，这时，必须切换到Ring0，然后进入3GB-4GB中的内核地址空间去执行这些代码来完成操作，完成后，切换回Ring3，回到用户态。这样，用户态的程序就不能随意操作内核地址空间，具有一定的安全保护作用。 为什么会有上下文系统的两种不同运行状态，才有了上下文的概念。CPU对上下文环境进一步细分，因此有：以下三种状态： 内核态，运行于进程上下文，内核代表进程运行于内核空间。内核态，运行于中断上下文，内核代表硬件运行于内核空间。用户态，运行于用户空间。 用户空间的应用程序，如果想请求系统服务，比如操作某个物理设备，映射设备的地址到用户空间，必须通过系统调用来实现。（系统调用是操作系统提供给用户空间的接口函数）。通过系统调用，用户空间的应用程序就会进入内核空间，由内核代表该进程运行于内核空间，这就涉及到上下文的切换，用户空间和内核空间具有不同的地址映射，通用或专用的寄存器组，而用户空间的进程要传递很多变量、参数给内核，内核也要保存用户进程的一些寄存器、变量等，以便系统调用结束后回到用户空间继续执行。交互如下图所示： 进程上下文所谓的进程上下文，就是一个进程在执行的时候，CPU的所有寄存器中的值、进程的状态以及堆栈上的内容，当内核需要切换到另一个进程时，它需要保存当前进程的所有状态，即保存当前进程的进程上下文，以便再次执行该进程时，能够恢复切换时的状态，继续执行。 一个进程的上下文可以分为三个部分：用户上下文、寄存器上下文以及系统级上下文： 用户级上下文：正文、数据、用户堆栈以及共享存储区；寄存器上下文：通用寄存器、程序寄存器（IP）、处理器状态寄存器（EFLAGS）、栈指针（ESP）；系统级上下文：进程控制块task_struct、内存管理信息（mm_struct、vm_area_struct、pgd、pte）、内核栈。 当发生进程调度时，进行进程切换就是上下文切换（context switch）。操作系统必须对上面提到的全部信息进行切换，新调度的进程才能运行。而系统调用运行的是模式切换（mode switch）。模式切换与进程切换比较起来，容易很多，而且节省时间，因为模式切换最主要的任务只是切换进程寄存器上下文的切换。在进程上下文中，可以用current宏关联当前进程，也可以睡眠，也可以调用调度程序。 中断上下文硬件通过触发信号，向CPU发送中断信号，导致内核调用中断处理程序，进入内核空间。这个过程中，硬件的一些变量和参数也要传递给内核，内核通过这些参数进行中断处理。所以，“中断上下文”就可以理解为硬件传递过来的这些参数和内核需要保存的一些环境，主要是被中断的进程的环境。内核进入中断上下文是因为中断信号而导致的中断处理或软中断。而中断信号的发生是随机的，中断处理程序及软中断并不能事先预测发生中断时当前运行的是哪个进程，所以在中断上下文中引用current是可以的，但没有意义。在中断上下文，通常都会始终占用CPU（当然中断可以嵌套，但我们一般不这样做），不可以被打断。不可以睡眠或者释放CPU。 为什么中断上下文不能调用可以睡眠的函数呢？中断的处理流程： 1.进入中断处理程序2.保存关键上下文3.开中断（sti指令）4.进入中断处理程序的handler5.关中断（cli指令）6.写EOI寄存器（表示中断处理完成）7.开中断 硬中断对应于上图中的1、2、3步骤，在这几个步骤中，所有中断是被屏蔽的，如果在这个时候睡眠了，操作系统不会收到任何中断（包括时钟中断），系统就基本处于瘫痪状态（例如调度器依赖的时钟节拍没有等等……） 软中断对应上图的4（当然，准确的说应该是4步骤的后面一点）。这个时候不能睡眠的关键是因为上下文。操作系统以进程调度为单位，进程运行在进程的上下文中，以进程描述符作为管理的数据结构task_struct。进程可以睡眠的原因是操作系统可以切换不同进程的上下文，进行调度操作，这些操作都以进程描述符为支持。中断运行在中断上下文，没有一个所谓的中断描述符来描述它，它不是操作系统调度的单位。一旦在中断上下文中睡眠， 首先无法切换上下文（因为没有中断描述符，当前上下文的状态得不到保存），其次，没有人来唤醒它，因为它不是操作系统的调度单位。此外，中断的发生是非常非常频繁的，在一个中断睡眠期间，其它中断发生并睡眠了，那很容易就造成中断栈溢出导致系统崩溃。如果上述条件满足了（也就是有中断描述符，并成为调度器的调度单位，栈也不溢出了，理论上是可以做到中断睡眠的），中断是可以睡眠的，但会引起很多问题：例如，你在时钟中断睡眠了，那操作系统的时钟就乱了，调度器也就失去了依据；例如，你在一个IPI（处理器间中断）中，其他CPU都在死循环等你答复，你却睡眠了，那其他处理器也不工作了；例如，你在一个DMA中断中睡眠了，上面的进程还在同步的等待I/O的完成，性能就大大降低了……还可以举出很多例子。所以，中断是一种紧急事务，需要操作系统立即处理，不是不能做到睡眠，是它没有理由睡眠。不过，在Linux调度器的具体实现的时候，检测到了在中断上下文中调度schedule函数也并没有强制Linux进入panic，可能是linux开发者认为一个好的内核调度器无论如何也尽自己最大的努力让系统运行下去吧。但是，在厂商自己提供的内核中，往往修改调度器行为，在中断上下文中检测到调度就直接panic了，对于内核开发者而言，这样更好，可以尽早的发现问题。]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>嵌入式</tag>
        <tag>操作系统</tag>
        <tag>LINUX</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[资源相关的编程模式]]></title>
    <url>%2F2021%2F05%2F20%2F%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E7%9A%84%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[本文主要总结资源相关的编程模式，以提高大家编程质量，减少遗漏释放资源的可能。 资源这里的资源是一个宽泛的概念。凡是需要成对操作的，都可以视为资源。比如动态内存、文件、锁、中断，等等。我们以函数为单位进行考察，编程中用到的资源大致分为三类，一类是本函数申请，无论本函数成功失败都要释放，例如不保存的动态内存、文件、锁、中断等等；第二类是本函数申请，无论本函数成功失败都不释放，例如发消息的内存；第三类是本函数申请，只有本函数失败才要释放，例如申请了表项资源，后续操作失败才需要回退。第二类从释放函数的角度，也可以看成本函数不申请，无论本函数成功失败都要释放。遗漏释放资源，是编程中经常出现的严重问题。其根本原因是单一函数的结构过于复杂、分支过多，在多个分支中需要考虑资源的释放，非常容易遗漏，尤其是在后期维护修改代码时。所以本文总结了两大类方法用以改善代码质量。 方法一：好的设计根本解决这类问题的方法在于降低函数的复杂度，所以对于新开发的特性在编码前设计时遵从以下原则： 资源与业务分离原则即通过把资源申请和释放之间的业务操作代码封装成函数的方法，将申请、释放资源的函数和使用资源的业务函数分离，使得业务函数中完全不必考虑资源释放的问题。如下图所示： 单一资源原则当一个函数中用到多个资源的时候，通过封装子函数的方法，使得一个函数中只出现一个资源，从而降低复杂度，减少出错的可能。如下图所示： 如果多个资源是并列关系，还可以把多个资源封装成一个大资源，从而在管理时视为一个资源。如下图所示： 唯一释放原则每个函数中只能有一处释放资源的代码。以多操作失败回退为例，通过函数封装使得每个函数中最多只有两步操作，因而只有一处失败回退的代码，从而避免遗漏。如下图所示： 释放规则统一申请资源的函数，对资源释放的规则要统一。当本函数退出时，无论处理成功还是失败，对申请的资源要么都由自己释放，要么不释放。不要出现成功不释放失败才释放的情况。对于发包流程中的内存申请所在的函数，建议设计为成功失败都不释放。如下图所示： 方法二：好的编码然而在实际 项目中，往往是对大量结构复杂的老代码的移植修改。在不允许遵照以上原则重写代码的场合，采用以下编程模式可以尽量减少遗漏释放资源。 1、函数只能有一个出口，使用宏DRV_SAFE_OUT安全退出，不得直接return。也可以直接用在DRV_SAFE_OUT基础上封装好的宏DRV_IF_ERR_SAFE_OUT或DRV_IF_ERR_ASSERT_SAFE_OUT（用于需要打印断言的场合）2、函数末尾必须有名为“SAFE_OUT”的goto标签，标签后的代码对本函数所有需要释放或回退的资源集中处理。 出口宏的封装Talk is cheap. Show you the code：对于有返回值的函数，宏封装如下：1234567891011121314151617181920212223/* 安全退出当前函数，函数返回值变量的命名自定，适用于功能性返回值 */#define DRV_SAFE_OUT(retName, retOut) \ (retName) = (retOut); \ goto SAFE_OUT /* 如果errCondition成立则打印信息并安全退出当前函数，函数返回值变量的命名自定，适用于功能性返回值 */#define DRV_IF_ERR_SAFE_OUT(errCondition, retName, retOut, icLevel, args...) \ if (errCondition) \ &#123; \ printk((icLevel)##args); \ (retName) = (retOut); \ goto SAFE_OUT; \ &#125; /* 如果errCondition成立则打印信息和断言并安全退出当前函数，函数返回值变量的命名自定，适用于功能性返回值 */#define DRV_IF_ERR_ASSERT_SAFE_OUT(errCondition, retName, retOut, icLevel, args...) \ if (errCondition) \ &#123; \ printk((icLevel)##args); \ assert(BOOL_FALSE); \ (retName) = (retOut); \ goto SAFE_OUT; \ &#125;对于无返回值的函数，宏封装如下：1234567891011121314151617181920/* 安全退出当前函数，函数返回值变量的命名自定，适用于功能性返回值 */#define DRV_SAFE_OUT2 \ goto SAFE_OUT /* 如果errCondition成立则打印信息并安全退出当前函数，适用于无返回值场合 */#define DRV_IF_ERR_SAFE_OUT2(errCondition, icLevel, args...) \ if (errCondition) \ &#123; \ printk((icLevel)##args); \ goto SAFE_OUT; \ &#125; /* 如果errCondition成立则打印信息和断言并安全退出当前函数，适用于无返回值场合 */#define DRV_IF_ERR_ASSERT_SAFE_OUT2(errCondition, icLevel, args...) \ if (errCondition) \ &#123; \ printk((icLevel)##args); \ assert(BOOL_FALSE); \ goto SAFE_OUT; \ &#125; 内存资源的释放带返回值的函数释放函数有返回值，无打印输出信息，如下图： 函数有返回值，有打印输出信息，如下图： 不带返回值的函数释放函数无返回值，无打印输出信息，如下图： 函数无返回值，有打印输出信息，如下图： 锁和信号量的释放实现分析此处也是该处总结的重点。为了统一释放锁和信号量，需要考虑如下几个问题： 1、一个函数里可能用到不止一个锁或者信号量，如何做到在函数出口全部都能释放呢？2、锁和信号量的种类很多，函数出口释放的时候如何知道你用哪个接口去释放呢？ 首先，要想释放一个锁（或信号量），必须要知道锁（或信号量）的地址以及解锁（释放信号量）函数。另外，对于spin_lock_irqsave(lock, flags)，获得自旋锁的同时把标志寄存器的值保存到变量flags中并失效本地中断。此时还需要一个存储寄存器值的变量。所以我们可以定义一个结构体： 1234567891011/* 各种锁(信号量)通用的函数原型 */typedef VOID (* DRV_LOCK_PF)(const VOID*, ULONG);/* 各种锁(信号量)通用的控制结构 */typedef struct tagDrvLock&#123; VOID *pLock; /* 锁的地址 */ DRV_LOCK_PF pfUnlock; /* 释放锁的函数 */ ULONG ulData; /* 释放锁的函数参数 */&#125;DRV_LOCK_S; 那么接下来，我们对所有锁（或信号量）的释放函数按照DRV_LOCK_PF的函数原型进行封装，如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/* 按照DRV_LOCK_PF原型对锁释放函数进行封装 */VOID DRV_SPIN_Unlock(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; spin_unlock((spinlock_t *)pLock); return;&#125;VOID DRV_WRITE_Unlock(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; write_unlock((rwlock_t *)pLock); return;&#125;VOID DRV_READ_Unlock(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; read_unlock((rwlock_t *)pLock); return;&#125;VOID DRV_READ_UnlockBh(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; read_unlock_bh((rwlock_t *)pLock); return;&#125;VOID DRV_WRITE_UnlockBh(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; write_unlock_bh((rwlock_t *)pLock); return;&#125;VOID DRV_SPIN_UnlockIrqRestore(const VOID *pLock, ULONG ulData)&#123; spin_unlock_irqrestore((spinlock_t *)pLock, ulData); return;&#125;VOID DRV_SPIN_UnlockBh(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; spin_unlock_bh((spinlock_t *)pLock); return;&#125;VOID DRV_Up(const VOID *pLock, ULONG ulData)&#123; (VOID)ulData; up((struct semaphore *)pSem); return;&#125; 接下来，就是确认函数使用锁（或信号量）的数量，为每个锁分配一个DRV_LOCK_S结构，用以释放。在函数内部实现定义变量时调用，如下：1234/* 声明本函数用到的锁的数量，包括各种锁和信号量 */#define DRV_DECLARE_LOCKS(lockCnt) \ DRV_LOCK_S astLocks_[lockCnt] = &#123;&#123;0&#125;&#125;; \ UINT uiLockCnt_ = 0 最后，我们就是要将锁（或信号量）和DRV_LOCK_S关联起来，加锁时入栈，解锁时出栈：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/* 已获取的锁(信号量)入栈 */#define DRV_LOCK_PUSH(stack, cnt, pLock, pfUnlock, data) \ stack[cnt].pLock = pLock; \ stack[cnt].pfUnlock = pfUnlock; \ stack[cnt].ulData = data; \ (cnt)++; /* 指定的锁(信号量)出栈，如果不是栈顶要搬移 */static inline VOID DRV_LOCK_Pop(IN const VOID *pLock, INOUT DRV_LOCK_S *pstStack, INOUT UINT *puiCnt)&#123; UINT uiTop = *puiCnt - 1; if (pLock != pstStack[uiTop].pLock) &#123; ULONG ulLoop; for (ulLoop = 0; ulLoop &lt; uiTop; ulLoop++) &#123; if (pLock == pstStack[uiLoop].pLock) &#123; memcpy(&amp;(pstStack[uiLoop]), &amp;(pstStack[ulLoop + 1]), (*puiCnt - (ulLoop + 1)) * sizeof(DRV_LOCK_S)); break; &#125; &#125; DBGASSERT(ulLoop &lt; uiTop); &#125; (*puiCnt)--; return;&#125;/* 和spin_lock用法一样 */#define DRV_SPIN_LOCK(pLock) \&#123; \ spin_lock(pLock); \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_SPIN_Unlock, 0); \&#125;/* 和spin_unlock用法一样 */#define DRV_SPIN_UNLOCK(pLock) \&#123; \ DRV_SPIN_Unlock(pLock, 0UL); \ DRV_LOCK_Pop(pLock, astLocks_, &amp;uiLockCnt_); \&#125;/* 和spin_lock_bh用法一样 */#define DRV_SPIN_LOCK_BH(pLock) \&#123; \ spin_lock_bh(pLock); DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_SPIN_UnlockBh, 0); \&#125;/* 和spin_unlock_bh用法一样 */#define DRV_SPIN_UNLOCK_BH(pLock) \&#123; \ DRV_SPIN_UnlockBh(pLock, 0UL); \ DRV_LOCK_Pop(pLock, astLocks_, &amp;uiLockCnt_); \&#125;/* 和spin_lock_irqsave用法一样 */#define DRV_SPIN_LOCK_IRQSAVE(pLock, flag) \&#123; \ spin_lock_irqsave(pLock, flag); DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_SPIN_UnlockIrqRestore, flag); \&#125;/* 和read_lock用法一样 */#define DRV_READ_LOCK(pLock) \&#123; \ read_lock(pLock); \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_READ_Unlock, 0); \&#125;/* 和read_unlock用法一样 */#define DRV_READ_UNLOCK(pLock) \&#123; \ DRV_READ_Unlock(pLock, 0UL); \ DRV_LOCK_Pop(pLock, astLocks_, &amp;uiLockCnt_); \&#125;/* 和read_lock_bh用法一样 */#define DRV_READ_LOCK_BH(pLock) \&#123; \ read_lock_bh(pLock); \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_READ_UnlockBh, 0); \&#125;/* 和read_unlock_bh用法一样 */#define DRV_READ_UNLOCK_BH(pLock) \&#123; \ DRV_READ_UnlockBh(pLock, 0UL); \ DRV_LOCK_Pop(pLock, astLocks_, &amp;uiLockCnt_); \&#125;/* 和write_lock用法一样 */#define DRV_WRITE_LOCK(pLock) \&#123; \ write_lock(pLock); \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_WRITE_Unlock, 0); \&#125;/* 和write_unlock用法一样 */#define DRV_WRITE_UNLOCK(pLock) \&#123; \ DRV_WRITE_Unlock(pLock, 0UL); \ DRV_LOCK_Pop(pLock, astLocks_, &amp;uiLockCnt_); \&#125;/* 和write_lock_bh用法一样 */#define DRV_WRITE_LOCK_BH(pLock) \&#123; \ write_lock_bh(pLock); \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pLock, DRV_WRITE_UnlockBh, 0); \&#125;/* 和write_unlock_bh用法一样 */#define DRV_WRITE_UNLOCK_BH(pLock) \&#123; \ DRV_WRITE_UnlockBh(pLock, 0UL); \ DRV_LOCK_Pop(pLock, astLocks_, &amp;uiLockCnt_); \&#125;/* 和down用法一样 */#define DRV_DOWN(pSem) \&#123; \ down(pSem); \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pSem, DRV_Up, 0); \&#125;/* 和up用法一样 */#define DRV_UP(pSem) \&#123; \ DRV_Up(pSem, 0UL); \ DRV_LOCK_Pop(pSem, astLocks_, &amp;uiLockCnt_); \&#125;/* 和down_timeout用法不同，返回值作为出参 */#define DRV_DOWN_TIMEOUT(pSem, time, ret) \ ret = down_timeout(pSem, time); if (0 == ret) \ &#123; \ DRV_LOCK_PUSH(astLocks_, uiLockCnt_, pSem, DRV_Up, 0); \ &#125; /* 释放本函数所有已获取的锁(信号量)，后进先出 */#define DRV_UNLOCK_ALL \&#123; \ UINT uiLoop, uiTop; \ for (uiLoop = 0; uiLoop &lt; uiLockCnt_; uiLoop++) &#123; \ uiTop = (uiLockCnt_ - 1) - uiLoop; \ astLocks_[uiTop].pfUnlock(astLocks_[uiTop].pLock, astLocks_[uiTop].ulData); \ &#125; \ uiLockCnt_ = 0; \&#125;]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>嵌入式</tag>
        <tag>C语言</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存寻址]]></title>
    <url>%2F2021%2F05%2F20%2F%E5%86%85%E5%AD%98%E5%AF%BB%E5%9D%80%2F</url>
    <content type="text"><![CDATA[最近学习《深入理解LINUX内核》内存寻址，本文对此知识做个总结。主要介绍80x86微处理器怎样进行芯片级的内存寻址，Linux(内核版本为2.6.11)又是如何利用寻址硬件的。 内存地址：逻辑地址：包含在机器语言指令中用来指定一个操作数或一条指令的地址。每一个逻辑地址都由一个段（segment）和偏移量（offset）组成。线性地址：32位无符号整数，可用来表示高达4GB的地址。范围是0x00000000到0xffffffff。物理地址：用于内存芯片级内存单元寻址。由32位或36位无符号整数表示。 内存控制单元（MMU）通过一种称为分段单元的硬件电路把一个逻辑地址转换成线性地址；接着，再通过分页单元的硬件电路把线性地址转换成一个物理地址。 硬件中的分段段标识符一个逻辑地址由两部分组成：一个段标识符和一个指定段内相对地址的偏移量。段标识符， 段标识符是由一个16位长的字段组成，称为段选择符。其中前13位是一个索引号。后面3位包含一些硬件细节。 为了快速方便地找到段选择符，处理器提供段寄存器，段寄存器的唯一目的是存放段选择符。这些段寄存器称为cs、ss、ds、es、fs和gs。尽管只有6个段寄存器，但程序可以把同一个段寄存器用于不同的目的，方法是先将其值保存在内存中，用完后再恢复。 下面三个段寄存器有专门的用途： cs：代码段寄存器，指向包含程序指令的段。ss：栈段寄存器，指向包含当前程序栈的段。ds：数据段寄存器，指向包含静态数据或者全局数据段。 其它三个段寄存器作一般用途，可以指向任意的数据段。 段描述符每个段由一个8字节的段描述符表示。它放在全局描述符（GDT）中或局部描述符（LDT）中。有几种不同类型的段和它们的段描述符如下： 理解： 段标识符的前13位，直接在段描述符表中找到一个具体的段描述符，这个描述符就描述了一个段。 快速访问段描述符每当一个段选择符被装入段寄存器时，会从段描述符表中选中一个段描述符，由内存装入到非编程CPU寄存器中。 Intel设计的本意是，一些全局的段描述符，就放在“全局段描述符表(GDT)”中，一些局部的，例如每个进程自己的，就放在所谓的“局部段描述符表(LDT)”中。那究竟什么时候该用GDT，什么时候该用LDT呢？这是由段选择符中的T1字段表示的，=0，表示用GDT，=1表示用LDT。 GDT在内存中的地址和大小存放在CPU的gdtr控制寄存器中，而LDT则在ldtr寄存器中。 由于段描述符是8字节长，因此它在GDT或LDT内的相对地址是由段选择符的最高13位的值乘以8得到的。例如：如果GDT在0x00020000，且由段选择符所指定的索引号为2，那么相应的段描述符的地址是0x00020000 + ( 2 x 8 ). 分段单元那么逻辑地址是怎样转换成线性地址的？如图所示 首先，给定一个完整的逻辑地址[段选择符：段内偏移地址]， 1、看段选择符的T1=0还是1，知道当前要转换是GDT中的段，还是LDT中的段，再根据相应寄存器，得到线性基地址。2、从段选择符的index字段（前13位）计算段描述符的地址，计算公式：index字段的值 x 8(一个段描述符的大小) + 上面的线性基地址。3、把逻辑地址的偏移量offset + 段描述符Base字段的值 = 线性地址。 以上都是基于80x86微处理器描述的，那么Linux是怎么分段的呢？ Linux中的分段Intel要求两次转换，这样虽说是兼容了，但是却是很冗余，呵呵，没办法，硬件要求这样做了，软件就只能照办，怎么着也得形式主义一样。另一方面，其它某些硬件平台，没有二次转换的概念，Linux也需要提供一个高层抽象，来提供一个统一的界面。所以，Linux的段式管理，事实上只是“哄骗”了一下硬件而已。 按照Intel的本意，全局的用GDT，每个进程自己的用LDT——不过Linux则对所有的进程都使用了相同的段来对指令和数据寻址。即用户数据段，用户代码段，对应的，内核中的是内核数据段和内核代码段。这样做没有什么奇怪的，本来就是走形式嘛，像我们写年终总结一样。 Linux只使用了四个主要的段类对指令和数据寻址。 段 Base G Limit S Type DPL D/B P 用户代码段 0x00000000 1 0xfffff 1 10 3 1 1 用户数据段 0x00000000 1 0xfffff 1 2 3 1 1 内核代码段 0x00000000 1 0xfffff 1 10 0 1 1 内核数据段 0x00000000 1 0xfffff 1 2 0 1 1 所有的段都是从0x00000000开始的，那就是说Linux下逻辑地址和线性地址是一致的。 即逻辑地址的偏移量字段的值与线性地址的值总是相同的。 硬件中的分页CPU的页式内存管理单元，负责把一个线性地址，最终翻译为一个物理地址。从管理和效率的角度出发，线性地址被分为以固定长度为单位的组，称为页(page)，分页单元把所有的RAM分成固定长度的页框（page frame）,每一个页框包含一个页，一个页框的长度和一个页的长度一致。把线性地址映射到物理地址的数据结构称为页表（page table）。 理解：页是一组线性地址，是指包含在这组地址地址中的数据块，可以存放在任何页框或磁盘中。页框是主存的一部分，是一个存储区域。页表是数据结构，存在在主存中。 常规分页32位的线性地址被分成3个域：Directory(目录)：最高10位 Table(页表)：中间10位 Offset(偏移量)：最低12位 使用二级模式的目的在于减少每个进程页表所需RAM的数量。如果使用一级页表，那将需要高达$2^{20}$（32位系统，线性地址最大为4GB，每个页4KB，可分成$2^{20}$个页，每个页的地址占4个字节，级需要4MB RAM来存储）个表项。 每个活动进程必须有一个分配给它的页目录。没有必要马上为进程的所有页表都分配RAM. 通过控制寄存器cr3获取页目录的物理地址（ 操作系统负责在调度进程的时候，把这个地址装入寄存器），通过Directory字段找到页目录中的目录项，目录项存放的是页表的基地址，通过Table字段找到页表表项，表项存放的是页框的物理地址。通过Offset字段又找到页框的相对位置。offset有12位长，故每一页框含有4096字节的数据。 Directory字段和Table字段都是10位长，因此页目录和页表都可以多达1024项。那么一个页目录可以寻址到高达$1024 1024 4096 = 2^{32}$个存储单元，这和你对32位地址所期望的一样。 常规分页举例假定内核给一个正在运行的进程分配的线性地址空间范围0x20000000到0x2003ffff。这个空间正好是是由64页组成（0x2003ffff - 0x20000000 + 1）/ 4K = 64)。这两个线性地址的最高10位（Directory字段）的值为0x080或十进制的128，即都指向进程页目录的第129项。该目录项中包含了分配给该进程的页表的物理地址。如果没有给这个进程分配其他的线性地址，则页目录的其余1023项都填0。中间的10位（Table字段）范围从0~0x03f，十进制即从0到63。因而只有页表的前64个表项是有意义的，其余960个表项都填0。 假设进程需要读线性地址0x20021406中的字节，步骤为： 1、Directory字段0x80选择页目录的第0x80目录项，该目录项存放的是页表的物理基地址2、Table字段0x21选择页表的第0x21表项，该表项存放的是页框的物理基地址3、最后，Offset字段0x406用于在目标页框中读偏移量位0x406中的字节 如果页表第0x21表项的Present标志为0，则此页就不在主存中。分页单元在线性地址转换的同时产生一个缺页异常。 扩展分页从Pentium模型开始，80x86引入了扩展分页，它允许页框大小为4MB而不是4KB。扩展分页用于把大段连续的线性地址转换成相应的物理地址，在这些情况下，内核可以不用中间页表进行地址转换，节省内存并保留TLB项。 通过设置页目录项的Page Size标志启用扩展分页功能。分页单元把32位线性地址分成两个字段：Directory(目录)：最高10位 Offset(偏移量)：其余22位 物理地址扩展（PAE）分页机制处理器所支持的RAM容量受连接到地址总线上的地址管脚数限制。Intel通过在它的处理器上把管脚数从32增加到36。让寻址能力从4G（$2^{32}$）提升到64G（$2^{36}$）。 1、64GB的RAM被分为$2^{24}$个页框，页表项的物理地址字段从20位扩展到了24位。 所以PAE的页表项为64位，所以一个4KB的页表只能包含512个表项。2、引入页目录指针表（Page Directory Pointer Table，PDPT）的页表新级别，它由4个64位表项组成。3、cr3控制寄存器包含一个27位的页目录指针表（PDPT）基地址字段。 未启用PAE下2M分页的页表结构： 启用PAE下4K分页的页表结构 线性地址的映射过程如下： 1、cr3：指向一个PDPT基地址2、地址的31～30：确定PDPT项3、地址的29～21：确定页目录项中的一个此处，发生了分支： A.如果页目录项的PS标志位等于0，那么页大小是4K 4）地址的20～12：确定页表的某一项5）地址的11～0：确定偏移B. 如果PS=1，启用大页4）地址的20～0：确定2M页中的偏移量。 总之，一旦cr3被设置，就可能寻址高达4GB RAM。如果我们希望对更多的RAM寻址，就必须在cr3中放置一个新值，或该表PDPT的内容。然而，使用PAE的主要问题是线性地址仍然32位长。这就迫使内核编程人员用同一线性地址映射到不同的RAM区。很明显，PAE并没有扩大进程的线性地址空间，因为它只处理物理地址。此外，只有内核能够修改进程的页表，所以在用户态下运行的进程不能使用大于4GB的物理地址空间。另一方面，PAE允许内核使用容量高达64GB的RAM，从而显著增加了系统中的进程数量。 64位系统中的分页32位处理器普遍采用两级分页。然而两级分页并不适用于采用64位系统的计算机。原因如下 :首先假设一个大小为4KB的标准页，4KB覆盖$2^{12}$个地址，所以offset字段是12位。如果我们现在决定仅仅使用64位中的48位来寻址(这个限制仍然能使我们自在地拥有256TB的寻址空间！)，剩下的48-12=36位被分配给Table和Directory字段。如果我们决定为两个字段各预留18位，那么每个进程的页目录和页表都含有$2^{18}$个项，即超过256000个项。由于这个原因，所有64位处理器的硬件分页系统都使用了额外的分页级别。使用的级别数量取决于处理器的类型。 平台名称 页大小 寻址使用位数 分页级别 线性地址分级 alpha 8KB 43 3 10+10+10+13 ia64 4KB 39 3 9+9+9+12 ppc64 4KB 41 3 10+10+9+12 x86_64 4KB 48 4 9+9+9+9+12 硬件高速缓存当今的微处理器时钟频率接近几个GHz，而动态RAM芯片的存取时间是时钟周期的数百倍。为此，80x86体系结构中引入了一个叫行(line)的新单位。行由几十个连续的字节组成，以脉冲突发模式在慢速DRAM和快速的静态RAM（SRAM）之间传送，用来实现高速缓存。 高速缓存单元插在分页单元和主内存之间。包含一个硬件高速缓存内存和一个高度缓存控制器：高速缓存内存：存放内存中真正的行。高速缓存控制器：存放一个表项数组，每个表项对应高速缓存内存中的一个行。 转换后援缓冲器（TLB）转换后援缓冲器（TLB）的高速缓存用于加快线性地址的转换。当一个线性地址第一次使用时，通过慢速访问RAM的页表计算出相应的物理地址。同时，物理地址被存放在一个TLB表项中，以便以后对同一个线性地址的引用可以快速地得到转换。 Linux中的分页Linux采用了一种同时适用于32位和64位系统的普通分页模型。直到2.6.10版本，Linux采用三级分页的模型。从2.6.11版本开始，采用了四级分页模型。这4种页表分别称为： 页全局目录PGD（对应X86的页目录）页上级目录PUD（新引进的）页中间目录PMD（新引进的）页表PT（对应X86的页表）。 页全局目录包含若干上级目录的地址，页上级目录又依次包含若干页中间目录的地址，而页中间目录又包含若干页表的地址。每一个页表项指向一个页框。线性地址因此被分成五个部分。图中没有显示位数，因为每一部分的大小与具体的计算机体系结构有关。 那么，对于使用二级管理架构32位的硬件，现在又是四级转换了，它们怎么能够协调地工作起来呢？嗯，来看这种情况下，怎么来划分线性地址吧！从硬件的角度，32位线性地址被分成了三部分——也就是说，不管软件怎么做，最终落实到硬件，也只认识这三位老大。从软件的角度，由于多引入了两部份，也就是说，共有五部分。——要让二层架构的硬件认识五部份也很容易，在地址划分的时候，将页上级目录和页中间目录的长度设置为0就可以了。这样，操作系统见到的是五部份，硬件还是按它死板的三部份划分，也不会出错，也就是说大家共建了和谐计算机系统。 同理，启用了物理地址扩展的32位系统使用了三级页表。Linux的页全局目录对应80x86的页目录指针表（PDPT），取消了页上级目录，页中间目录对应80x86的页目录，Linux的页表对应80x86的页表。 这样，虽说是多此一举，但是考虑到64位地址，使用四层转换架构的CPU，我们就不再把中间两个设为0了，这样，软件与硬件再次和谐——抽像就是强大呀！！！ 每一个进程都有它自己的页全局目录和自己的页表集。当发生进程切换时，Linux把cr3控制寄存器的内容保存在前一个执行进程的描述符中，然后把下一个要执行进程的描述符的值装入cr3寄存器中。因此，当进程重新开始在CPU上执行时，分页单元指向一组正确的页表。 物理内存布局在初始化阶段，内核必须建立一个物理地址映射来指定哪些物理地址范围内对内核可用而哪些不可用。 内核将下列页框记为保留，保留页框中的页绝不能被动态分配或交换到磁盘上： 1、在不可用的物理地址范围内的页框2、含有内核代码和已初始化的数据结构的页框 一般来说，Linux内核安装在RAM中从物理地址0x00100000开始的地方，也就是说，从第二个MB开始。为什么内核没有安装在RAM第一个MB开始的地方？因为PC体系结构有几个独特的地方必须考虑到： 1、页框0由BIOS使用，存放加电自检（POST）期间检查到的系统硬件配置。2、物理地址从0x000a0000到0x000fffff的范围通常留给BIOS例程，并且映射ISA图形卡上的内部内存。3、第一个MB内的其他页框可能由特定计算机模型保留。 为了避免把内核装入一组不连续的页框里，Linux更愿意跳过RAM的第一个MB。明确地说，Linux把PC体系结构未保留的页框来动态存放所分配的页。 下图显示Linux怎样填充前3MB的RAM。我们假设内核需要小于3MB的RAM。符号_text对应于物理地址0x00100000，表示内核代码第一个字节的地址。符号_etext表示内核代码的结束位置，初始化内核数据的开始位置。符号_edata表示初始化的内核数据结束位置，未初始化内核数据的开始位置。符号_end表示未初始化内核的数据结束位置。 进程页表进程的地址空间分为两部分：从0x00000000到0xbfffffff的（3G，用户空间）线性地址，无论进程运行在用户态还是内核态都可以寻址。从0xc0000000到0xffffffff的（1G，内核空间）线性地址，只有内核态的进程才能寻址。 当进程运行在用户态时，它产生的线性地址小于0xc0000000；当进程运行在内核态时，它执行内核代码，所产生的地址大于等于0xc0000000，但是，在某些情况下，内核为了检索或者存放数据必须访问用户态线性地址空间。 页全局目录的第一部分表项映射的线性地址小于0xc0000000（在PAE未启用时是前768项，PAE启用时是前3项），具体大小依赖于特定进程。在0xc0000000之后的表项对所有进程来说都应该是相同的，他们等于主内核页全局目录的相应表项。 内核页表内核维持着一组自己使用的页表，驻留在所谓的主内核页全局目录中。内核如何初始化自己的页表，这个过程分为两个阶段。第一阶段，内核创建一个有限的地址空间。这个最小限度的地址空间仅够将内核装入RAM和对其初始化的核心数据结构。第二阶段，内核充分利用剩余的RAM并适当地建立分页表。 临时内核页表临时内核页全局目录是在内核编译过程中静态初始化的，而临时页表是有startup_32()汇编语言函数初始化的。 临时页全局目录放在swapper_pg_dir变量中。临时页表在pg0变量处开始存放，紧接在内核未初始化的数据段后面。 分页的第一个阶段的目标是允许在实模式下和保护模式下都能很容易得对内核寻址，假定是8MB。 startup_32()函数通过向cr3控制寄存器装入swapper_pg_dir的地址及设置cr0控制寄存器的PG标志启用分页单元。 当RAM小于896MB时的最终内核页表由内核页表所提供的最终映射必须把从0xc00000000开始的线性地址转化为从0开始的物理地址。宏pa用于把从PAGE_OFFSET开始的线性地址转换成相应的物理地址，宏va做相反的转化。 主内核页全局目录仍然保存在swapper_pg_dir变量中由paging_init()函数初始化。 由startup_32()函数创建的物理内存前8MB的恒等映射用来完成内核的初始化阶段，当这种映射不再必要时，内核调用zap_low_mappings()函数来清除对应的页表项。 当RAM大小在896MB和4096MB之间时的最终内核页表linux在初始化阶段可以做的最好的事就是把一个具有896MB的RAM窗口映射到内核线性地址空间。如果一个程序需要对现有的RAM的其余部分寻址，那就必须把某些其他的线性地址间隔映射到所需的RAM，即剩余的RAM留着不映射由动态映射来处理。 当RAM大于4096MB时的最终内核页表页全局目录中的前三项与用户线性地址空间相对应，内核用一个空页的地址对这三项进行初始化。第四项用页中间目录的地址初始化，该页中间目录是通过调用alloc_bootmen_low_pages()分配的。页中间目录的前448项用RAM前896MB的物理地址填充。 然后页全局目录的第四项被拷贝到第一项中，这样好为线性地址的前896MB中的低物理内存映射做镜像。为了完成SMP系统的初始化，这个映射是必须的：当这个映射不再必要时，内核通过调用zap_low_mappings()来清除对应的页表项。 固定映射的线性地址我们看到内核线性地址第四个GB的初始部分映射系统的物理内存。但是，至少128MB的线性地址总是留作他用，因为内核使用这些线性地址实现非连续内存分配和固定映射的线性地址。 内核线性地址空间范围：3GB-4GB (0xc0000000-0xffffffff)内核线性地址空间[3GB,3GB+896MB]————-（线性映射）—————物理地址空间[0,896M]内核线性地址空间[3GB+896MB,4GB]用来实现“非连续内存分配”和“固定映射” 固定映射的线性地址以“任意方式”（与前896MB线性映射方式相比）映射任何物理地址空间，固定映射使用的线性地址位于线性地址第4个GB末端 处理硬件高速缓存和TLB处理硬件高速缓存硬件高速缓存是通过高速缓存行寻址的。为了使高速缓存的命中率达到最优化： 1、一个数据结构中最常使用的字段放在该数据结构内的低偏移部分，以便它们能给处于高速缓存的同一行中。2、当为一大组数据结构分配空间时，内核试图把它们都存放在内存中，以便所有高速缓存行按同一方式使用。 处理TLBTLB作用页表一般都很大，并且存放在内存中，所以处理器引入MMU后，读取指令、数据需要访问两次内存：首先通过查询页表得到物理地址，然后访问该物理地址读取指令、数据。为了减少因为MMU导致的处理器性能下降，引入了TLB，TLB是Translation Lookaside Buffer的简称，可翻译为“地址转换后援缓冲器”，也可简称为“快表”。简单地说，TLB就是页表的Cache，其中存储了当前最可能被访问到的页表项，其内容是部分页表项的一个副本。只有在TLB无法完成地址翻译任务时，才会到内存中查询页表，这样就减少了页表查询导致的处理器性能下降。TLB中的项由两部分组成：标识和数据。标识中存放的是虚地址的一部分，而数据部分中存放物理页号、存储保护信息以及其他一些辅助信息。 TLB原理当cpu要访问一个虚拟地址/线性地址时，CPU会首先根据虚拟地址的高20位（20是x86特定的，不同架构有不同的值）在TLB中查找。如果是表中没有相应的表项，称为TLB miss，需要通过访问慢速RAM中的页表计算出相应的物理地址。同时，物理地址被存放在一个TLB表项中，以后对同一线性地址的访问，直接从TLB表项中获取物理地址即可，称为TLB hit。 想像一下x86_32架构下没有TLB的存在时的情况，对线性地址的访问，首先从PGD中获取PTE（第一次内存访问），在PTE中获取页框地址（第二次内存访问），最后访问物理地址，总共需要3次RAM的访问。如果有TLB存在，并且TLB hit，那么只需要一次RAM访问即可。 TLB表项TLB内部存放的基本单位是页表条目，对应着RAM中存放的页表条目。页表条目的大小固定不变的，所以TLB容量越大，所能存放的页表条目越多，TLB hit的几率也越大。但是TLB容量毕竟是有限的，因此RAM页表和TLB页表条目无法做到一一对应。因此CPU收到一个线性地址，那么必须快速做两个判断： 1、所需的页表是否已经缓存在TLB内部（TLB miss或者TLB hit）2、所需的页表在TLB的哪个条目内 为了尽量减少CPU做出这些判断所需的时间，那么就必须在TLB页表条目和内存页表条目之间的对应方式做足功夫 全相连 - full associative在这种组织方式下，TLB cache中的表项和线性地址之间没有任何关系，也就是说，一个TLB表项可以和任意线性地址的页表项关联。这种关联方式使得TLB表项空间的利用率最大。但是延迟也可能相当的大，因为每次CPU请求，TLB硬件都把线性地址和TLB的表项逐一比较，直到TLB hit或者所有TLB表项比较完成。特别是随着CPU缓存越来越大，需要比较大量的TLB表项，所以这种组织方式只适合小容量TLB 直接匹配每一个线性地址块都可通过模运算对应到唯一的TLB表项，这样只需进行一次比较，降低了TLB内比较的延迟。但是这个方式产生冲突的几率非常高，导致TLB miss的发生，降低了命中率。 比如，我们假定TLB cache共包含16个表项，CPU顺序访问以下线性地址块：1, 17 , 1, 33。当CPU访问地址块1时，1 mod 16 = 1，TLB查看它的第一个页表项是否包含指定的线性地址块1，包含则命中，否则从RAM装入；然后CPU访问地址块17，17 mod 16 = 1，TLB发现它的第一个页表项对应的不是线性地址块17，TLB miss发生，TLB访问RAM把地址块17的页表项装入TLB；CPU接下来访问地址块1，此时又发生了miss，TLB只好访问RAM重新装入地址块1对应的页表项。因此在某些特定访问模式下，直接匹配的性能差到了极点 组相连 - set-associative 为了解决全相连内部比较效率低和直接匹配的冲突，引入了组相连。这种方式把所有的TLB表项分成多个组，每个线性地址块对应的不再是一个TLB表项，而是一个TLB表项组。CPU做地址转换时，首先计算线性地址块对应哪个TLB表项组，然后在这个TLB表项组顺序比对。按照组长度，我们可以称之为2路，4路，8路。 经过长期的工程实践，发现8路组相连是一个性能分界点。8路组相连的命中率几乎和全相连命中率几乎一样，超过8路，组内对比延迟带来的缺点就超过命中率提高带来的好处了。 这三种方式各有优缺点，组相连是个折衷的选择，适合大部分应用环境。当然针对不同的领域，也可以采用其他的cache组织形式。 TLB表项更新TLB表项更新可以有TLB硬件自动发起，也可以有软件主动更新 1、TLB miss发生后，CPU从RAM获取页表项，会自动更新TLB表项2、TLB中的表项在某些情况下是无效的，比如进程切换，更改内核页表等，此时CPU硬件不知道哪些TLB表项是无效的，只能由软件在这些场景下，刷新TLB。 在linux kernel软件层，提供了丰富的TLB表项刷新方法，但是不同的体系结构提供的硬件接口不同。比如x86_32仅提供了两种硬件接口来刷新TLB表项： 1、向cr3寄存器写入值时，会导致处理器自动刷新非全局页的TLB表项2、在Pentium Pro以后，invlpg汇编指令用来无效指定线性地址的单个TLB表项无效。 参考《深入理解LINUX内核》物理地址扩展（PAE）分页机制TLB的作用及工作原理]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>LINUX</tag>
        <tag>OS</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间复杂度和空间复杂度]]></title>
    <url>%2F2021%2F05%2F20%2F%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%92%8C%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[数据结构和算法本身解决的是“快”和“省”的问题，即如何让代码运行得更快，如何让代码更省存储空间。所以，执行效率是算法一个非常重要的考量指标。那如何来衡量你编写的算法代码的执行效率呢？这里就要用到我们今天要讲的内容：时间、空间复杂度分析。 为什么需要复杂度分析通常，对于一个给定的算法，我们要做两项分析。第一是从数学上证明算法的正确性，这一步主要用到形式化证明的方法及相关推理模式，如循环不变式、数学归纳法等。而在证明算法是正确的基础上，第二部就是分析算法的时间复杂度。算法的时间复杂度反映了程序执行时间随输入规模增长而增长的量级，在很大程度上能很好反映出算法的优劣与否。因此，作为程序员，掌握基本的算法时间复杂度分析方法是很有必要的。 算法执行时间需通过依据该算法编制的程序在计算机上运行时所消耗的时间来度量。而度量一个程序的执行时间通常有两种方法。 事后统计的方法这种方法主要是通过设计好的测试程序和数据，利用计算机计时器对不同算法编制的程序的运行时间进行比较，从而确定算法效率的高低。但这种方法显然是有很大缺陷的： (1). 必须依据算法事先编制好程序，这通常需要花费大量的时间和精力。如果编制出来发现它根本是很糟糕的算法，不是竹篮打水一场空吗？ (2). 时间的比较依赖计算机硬件和软件等环境因素，有时会掩盖算法本身的优劣。要知道，现在的一台四核处理器的计算机，跟当年286、386、486等老爷爷辈的机器相比，在处理算法的运算速度上，是不能相提并论的；而所用的操作系统、编译器、运行框架等软件的不同，也可以影响它们的结果；就算是同一台机器，CPU使用率和内存占用情况不一样，也会造成细微的差异。 (3). 算法的测试数据设计困难，并且程序的运行时间往往还与测试数据的规模有很大关系，效率高的算法在小的测试数据面前往往得不到体现。比如10个数字的排序，不管用什么算法，差异几乎是零。而如果有一百万个随机数字排序，那不同算法的差异就非常大了。那么我们为了比较算法，到底用多少数据来测试，这是很难判断的问题。 基于事后统计方法有这样那样的缺陷，我们考虑不予采纳。 事前分析估算的方法因事后统计方法更多的依赖于计算机的硬件、软件等环境因素，有时容易掩盖算法本身的优劣。因此人们常常采用事前分析估算的方法。 在编写程序前，依据统计方法对算法进行估算。一个用高级语言编写的程序在计算机上运行时所消耗的时间取决于下列因素： (1). 算法采用的策略、方法；(2). 编译产生的代码质量；(3). 问题的输入规模；(4). 机器执行指令的速度。 一个算法是由控制结构（顺序、分支和循环3种）和原操作（指固有数据类型的操作）构成的，则算法时间取决于两者的综合效果。为了便于比较同一个问题的不同算法，通常的做法是，从算法中选取一种对于所研究的问题（或算法类型）来说是基本操作的原操作，以该基本操作的重复执行的次数作为算法的时间量度。 函数的渐近增长算法对比1 我们现在来判断一下，以下两个算法A和B哪个更好。假设两个算法的输入规模都是n，算法A要做2n+3次操作，你可以理解为先有一个n次的循环，执行完成后，再有一个n次循环，最后有三次赋值或运算，共2n+3次操作。算法B要做3n+1次操作。你觉得它们谁更快呢？准确说来，答案是不一定的，如下图所示。 当n=1时，算法A效率不如算法B（次数比算法B要多一次）。而当n=2时，两者效率相同；当n&gt;2时，算法A就开始优于算法B了，随着n的增加，算法A比算法B越来越好了（执行的次数比B要少）。于是我们可以得出结论，算法A总体上要好过算法B。 此时我们给出这样的定义，输入规模n在没有限制的情况下，只要超过一个数值N，这个函数就总是大于另一个函数，我们称函数是渐近增长的。函数的渐近增长：给定两个函数f(n)和g(n)，如果存在一个整数N，使得对于所有的n&gt;N，f(n)总是比g(n)大，那么，我们说f(n)的增长渐近快于 从中我们发现，随着n的增大，后面的+3还是+1其实是不影响最终的算法变化的，例如算法A′与算法B′，所以，我们可以忽略这些加法常数项。 算法对比2我们来看第二个例子，算法C是4n+8，算法D是$2n^2+1$，如下图所示。 当n≤3的时候，算法C要差于算法D（因为算法C次数比较多），但当n&gt;3后，算法C的优势就越来越优于算法D了，到后来更是远远胜过。而当后面的常数去掉后，我们发现其实结果没有发生改变。甚至我们再观察发现，哪怕去掉与n相乘的常数，这样的结果也没发生改变，算法C′的次数随着n的增长，还是远小于算法D′。也就是说，与最高次项相乘的常数并不重要。 算法对比3我们再来看第三个例子。算法E是$2n^2+3n+1$，算法F是$2n^3+3n+1$，如下图所示。 当n=1的时候，算法E与算法F结果相同，但当n&gt;1后，算法E的优势就要开始优于算法F，随着n的增大，差异非常明显。通过观察发现，最高次项的指数大的，函数随着n的增长，结果也会变得增长特别快。 算法对比4我们来看最后一个例子。算法G是$2n^2$，算法H是3n+1，算法I是$2n^2+3n+1$，如下图所示。 这组数据应该就看得很清楚。当n的值越来越大时，你会发现，3n+1已经没法和2n2的结果相比较，最终几乎可以忽略不计。也就是说，随着n值变得非常大以后，算法G其实已经很趋近于算法I。于是我们可以得到这样一个结论，判断一个算法的效率时，函数中的常数和其他次要项常常可以忽略，而更应该关注主项（最高阶项）的阶数。 判断一个算法好不好，我们只通过少量的数据是不能做出准确判断的。根据刚才的几个样例，我们发现，如果我们可以对比这几个算法的关键执行次数函数的渐近增长性，基本就可以分析出：某个算法，随着n的增大，它会越来越优于另一算法，或者越来越差于另一算法。这其实就是事前估算方法的理论依据，通过算法时间复杂度来估算算法时间效率。 算法的时间复杂度算法的时间复杂度的定义在进行算法分析时，语句总的执行次数T(n)是关于问题规模n的函数，进而分析T(n)随n的变化情况并确定T(n)的数量级。算法的时间复杂度，也就是算法的时间量度，记作：T(n)=O(f(n))。它表示随问题规模n的增大，算法执行时间的增长率和f(n)的增长率相同，称作算法的渐近时间复杂度，简称为时间复杂度。其中f(n)是问题规模n的某个函数。 这样用大写O( )来体现算法时间复杂度的记法，我们称之为大O记法。一般情况下，随着n的增大，T(n)增长最慢的算法为最优算法。O(1)叫常数阶、O(n)叫线性阶、O($n^2$)叫平方阶，当然，还有其他的一些阶，我们之后会介绍。 推导大O阶方法那么如何分析一个算法的时间复杂度呢？即如何推导大O阶呢？我们给出了下面的推导方法，基本上，这也就是总结前面我们举的例子。推导大O阶： 1．用常数1取代运行时间中的所有加法常数。2．在修改后的运行次数函数中，只保留最高阶项。3．如果最高阶项存在且不是1，则去除与这个项相乘的常数。 常见时间复杂度常数阶123int sum = 0 ; n = 100; /*执行一次*/sum = (1+n)*n/2; /*执行一次*/printf(&quot;%d&quot;,sum); /*执行一次*/ 这个算法的运行次数函数是f(n)=3。根据我们推导大O阶的方法，第一步就是把常数项3改为1。在保留最高阶项时发现，它根本没有最高阶项，所以这个算法的时间复杂度为O(1)。 另外，我们试想一下，如果这个算法当中的语句sum=(1+n)*n/2有10句，即：123456789101112int sum = 0 ; n = 100; /*执行一次*/sum = (1+n)*n/2; /*执行第1次*/sum = (1+n)*n/2; /*执行第2次*/sum = (1+n)*n/2; /*执行第3次*/sum = (1+n)*n/2; /*执行第4次*/sum = (1+n)*n/2; /*执行第5次*/sum = (1+n)*n/2; /*执行第6次*/sum = (1+n)*n/2; /*执行第7次*/sum = (1+n)*n/2; /*执行第8次*/sum = (1+n)*n/2; /*执行第9次*/sum = (1+n)*n/2; /*执行第10次*/printf(&quot;%d&quot;,sum); /*执行一次*/ 事实上无论n为多少，上面的两段代码就是3次和12次执行的差异。这种与问题的大小无关（n的多少），执行时间恒定的算法，我们称之为具有O(1)的时间复杂度，又叫常数阶。注意：不管这个常数是多少，3或12，都不能写成O(3)、O(12)，而都要写成O(1)，这一点要特别注意。此外，对于分支结构而言，无论真假执行的次数都是恒定不变的，不会随着n的变大而发生变化，所以单纯的分支结构（不在循环结构中），其时间复杂度也是O(1)。 线性阶线性阶的循环结构会复杂很多。要确定某个算法的阶次，我们常常需要确定某个特定语句或某个语句集运行的次数。因此，我们要分析算法的复杂度，关键就是要分析循环结构的运行情况。下面这段代码，它的循环的时间复杂度为O(n)，因为循环体中的代码须要执行n次。 12345int i;for (i = 0; i &lt; n; i++)&#123; /* 时间复杂度为O(1)的程序步骤序列 */&#125; 对数阶下面的这段代码，时间复杂度又是多少呢？123456int count = 1;while (count &lt; n)&#123; count = count * 2; /* 时间复杂度为O(1)的程序步骤序列 */&#125;由于每次count乘以2之后，就距离n更近了一分。也就是说，有多少个2相乘后大于n，则会退出循环。由$2^x=n$得到$x=\log_2n$。所以这个循环的时间复杂度为O($\log n$)。 平方阶下面例子是一个循环嵌套，它的内循环刚才我们已经分析过，时间复杂度为O(n)。 12345678int i, j;for (i = 0; i &lt; n; i++)&#123; for (j = 0; j &lt; n; j++) &#123; /* 时间复杂度为O(1)的程序步骤序列 */ &#125;&#125; 而对于外层的循环，不过是内部这个时间复杂度为O(n)的语句，再循环n次。所以这段代码的时间复杂度为O($n^2$)。 如果外循环的循环次数改为了m，时间复杂度就变为O(m×n)。12345678int i, j;for (i = 0; i &lt; m; i++)&#123; for (j = 0; j &lt; n; j++) &#123; /* 时间复杂度为O(1)的程序步骤序列 */ &#125;&#125;所以我们可以总结得出，循环的时间复杂度等于循环体的复杂度乘以该循环运行的次数。 那么下面这个循环嵌套，它的时间复杂度是多少呢？ 123456789int i, j;for (i = 0; i &lt; n; i++)&#123; /* 注意j = i 而不是0 */ for (j = i; j &lt; n; j++) &#123; /* 时间复杂度为O(1)的程序步骤序列 */ &#125;&#125; 由于当i=0时，内循环执行了n次，当i=1时，执行了n-1次，……当i=n-1时，执行了1次。所以总的执行次数为：$n+(n-1)+(n-1)+…+1 = n(n+1)/2 = n^2/2 + n/2$用我们推导大O阶的方法，第一条，没有加法常数不予考虑；第二条，只保留最高阶项，因此保留n2/2；第三条，去除这个项相乘的常数，也就是去除1/2，最终这段代码的时间复杂度为O($n^2$)。 常见的时间复杂度 执行次数函数 阶 术语描述 12 O(1) 常数阶 2n+3 O(n) 线性阶 3n2 +2n+1 O($n^2$ ) 平方阶 5log2 n+20 O($\log n$) 对数阶 2n+3nlog2 n+19 O($n\log n$) nlogn阶 6n3+2n2 +3n+4 O($n^3$ ) 立方阶 2n O($2^n$ ) 指数阶 常用的时间复杂度所耗费的时间从小到大依次是：$O(1)&lt;O(\log n)&lt;O(n)&lt;O(n\log n)&lt;O(n^2)&lt;O(n^3)&lt;O(2^n)&lt;O(n!)&lt;O(n^n)$ 算法的空间复杂度算法的空间复杂度定义 一个程序的空间复杂度是指运行完一个程序所需内存的大小，利用程序的空间复杂度，可以对程序的运行所需要的内存多少有个预先估计。一个程序执行时除了需要存储空间和存储本身所使用的指令、常数、变量和输入数据外，还需要一些对数据进行操作的工作单元和存储一些为现实计算所需信息的辅助空间。程序执行时所需存储空间包括以下两部分： （1）固定部分：这部分空间的大小与输入/输出的数据的个数多少、数值无关，主要包括指令空间（即代码空间）、数据空间（常量、简单变量）等所占的空间，这部分属于静态空间。（2）可变空间：这部分空间的主要包括动态分配的空间，以及递归栈所需的空间等，这部分的空间大小与算法有关。 我们在写代码时，完全可以用空间来换取时间，比如说，要判断某某年是不是闰年，你可能会花一点心思写了一个算法，而且由于是一个算法，也就意味着，每次给一个年份，都是要通过计算得到是否是闰年的结果。还有另一个办法就是，事先建立一个有2050个元素的数组（年数略比现实多一点），然后把所有的年份按下标的数字对应，如果是闰年，此数组项的值就是1，如果不是值为0。这样，所谓的判断某一年是否是闰年，就变成了查找这个数组的某一项的值是多少的问题。此时，我们的运算是最小化了，但是硬盘上或者内存中需要存储这2050个0和1。这是通过一笔空间上的开销来换取计算时间的小技巧。到底哪一个好，其实要看你用在什么地方。 算法的空间复杂度通过计算算法所需的存储空间实现，算法空间复杂度的计算公式记作：S(n)=O(f(n))，其中，n为问题的规模，f(n)为语句关于n所占存储空间的函数。 常见的空间复杂度常数阶 12345678910int fun(int n)&#123; int i，j，k，s; s=0; for (i=0;i&lt;=n;i++) for (j=0;j&lt;=i;j++) for (k=0;k&lt;=j;k++) s++; return(s);&#125; 由于算法中临时变量得个数与问题规模n无关，所以空间复杂度均为S(n) = O(1) 线性阶 1234567891011121314151617void fun(int a[]，int n，int k) //数组a共有n个元素&#123; int i; if (k==n-1) &#123; for (i=0;i&lt;n;i++) printf(“%d\n”，a[i]); //执行n次 &#125; else &#123; for (i=k;i&lt;n;i++) a[i]=a[i]+i*i; //执行n-k次 fun(a，n，k+1); &#125;&#125; 此方法属于递归算法，每次调用本身都要分配空间，fun(a,n,0)的空间复杂度为O(n)。S(n) = O(g(1*n))若写成非递归算法，代码一般可能比较长，算法本身占用的存储空间较多，但运行时将可能需要较少的存储单元。 常见数据结构的空间复杂度 时间复杂度、空间复杂度举例分析二分法查找二分查找的非递归算法123456789101112131415161718192021222324template&lt;typename T&gt; T* BinarySearch(T* array,int number,const T&amp; data) //data要查找的数，number查找范围长度，array要查找的数组&#123; assert(number&gt;=0); int left = 0; int right = number-1; while (right &gt;= left) &#123; int mid = (left&amp;right) + ((left^right)&gt;&gt;1); if (array[mid] &gt; data) &#123; right = mid - 1; &#125; else if (array[mid] &lt; data) &#123; left = mid + 1; &#125; else &#123; return (array + mid); &#125; &#125; return NULL; &#125; 分析：假设最坏情况下，循环X次之后找到，则：$2^x=n; x=\log_2n$循环的基本次数是$\log_2N$，所以: 时间复杂度是O(logN);由于辅助空间是常数级别的所以：空间复杂度是O(1); 二分查找的递归算法123456789101112131415161718template&lt;typename T&gt; T* BinarySearch(T* left,T* right,const T&amp; data) &#123; assert(left); assert(right); if (right &gt;=left) &#123; T* mid =left+(right-left)/2; if (*mid == data) return mid; else return *mid &gt; data ? BinarySearch(left, mid - 1, data) : BinarySearch(mid + 1, right, data); &#125; else &#123; return NULL; &#125; &#125; 假设最坏情况下，循环X次之后找到，则：$2^x=n; x=\log_2n$递归的次数和深度都是$\log_2N$,每次所需要的辅助空间都是常数级别的：时间复杂度:O(log2N)；空间复杂度：O(log2N )。 斐波那契数这里我借用百度百科上的解释：斐波那契数，亦称之为斐波那契数列（意大利语： Successione di Fibonacci)，又称黄金分割数列、费波那西数列、费波拿契数、费氏数列，指的是这样一个数列：0、1、1、2、3、5、8、13、21、……在数学上，斐波纳契数列以如下被以递归的方法定义：F0=0，F1=1，Fn=Fn-1+Fn-2（n&gt;=2，n∈N*），用文字来说，就是斐波那契数列列由 0 和 1 开始，之后的斐波那契数列系数就由之前的两数相加。特别指出：0不是第一项，而是第零项。在这个斐波那契数列中的数字，就被称为斐波那契数求第N个斐波那契数比较简单可以直接套用公式n = 0,1 时，fib(n) = 1；n &gt; =2 时，fib(n) = fib(n-2) + fib(n-1)在计算时有两种算法：递归和非递归。如下： 第N个斐波那契数的非递归算法12345678910111213141516171819202122232425//非递归算法long long fib1(size_t N) &#123; long long a = 0, b = 1, c = 0; if (N &lt; 2) &#123; return N; &#125; else &#123; for (long long i = 2; i &lt;= N; ++i) &#123; c = a + b; a = b; b = c; &#125; &#125; return c;&#125;int main()&#123; printf(&quot;%lld&quot;, fib1(10)); getchar(); return 0;&#125; //此算法最大的优点是不存在重复计算，故效率比递归算法快的多得多。 使用非递归算法求到第n个斐波那契数，是从第2个数开始，将前两个数相加求求后一个数，再将后一个数赋值给前一个数，再计算前两个数相加的结果。依次类推直到第n个数，用n-1个数和n-2个数相加求出结果，这样的好处是，我们只计算了n-1次就求出了结果，即时间复杂度为O(n)；我们以代码中测试数10为例详解求第十个数的过程。当N=10时，进入函数首先判断，然后走下面的分支开始计算。 计算了N-1次，得出了结果所以时间复杂度是O（N）。此函数内部最多时一共开辟了a, b, c, i四个变量空间复杂度是常数，即为O（1）。 第N个斐波那契数的递归算法12345678910111213//递归算法long long fib2(size_t N) &#123; if (N &lt; 2) return N; return fib2(N - 1) + fib2(N - 2);&#125;int main()&#123; printf(&quot;%lld&quot;, fib2(10)); getchar(); return 0;&#125; 在递归算法中，求解fib2(n)，把它推到求解fib2(n-1)和fib2(n-2)。也就是说，为计算fib2(n)，必须先计算fib2(n-1)和fib2(n-2)，而计算fib2(n-1)和fib2(n-2)，时按照表达式及计算法则，需先计算又必须先计算fib2(n-1)，而fib2(n-1)由fib2(n-2)和fib2(n-3)计算得来，而这之中的和fib2(n-2)由fib2(n-3)和fib2(n-4)计算得来……依次类推，表面上看不出有何复杂度，但是仔细分析可知，每一个计算fib2(n)的分支都会衍生出计算直至(1)和fib(0)，也就是说每个分支都要自己计算数本身到1的斐波那契数列，这样就增加了庞大且冗杂的运算量，还是以10 为例详细计算说明。 图中数字代表第N个斐波那契数，图中没有全部将计算步骤画出来，但是已经足够说明问题，它的每一步计算都被分成计算前两个斐波那契数，以此类推。那么这就形成了一颗二叉树，虽然不是满二叉树，但是我们分析的是最坏时间复杂度，而且只要估算出来递归次数随N增长的趋势即可，故可以近似将它看成满二叉树，其中的节点数就是计算的次数，也就是复杂度，由公式：节点数=$2^h-1$（h为树的高度）可得O（$2^n$）。递归的时间复杂度是： 递归次数*每次递归中执行基本操作的次数，所以时间复杂度是： O($2^N$) 递归最深的那一次所耗费的空间足以容纳它所有递归过程。递归产生的栈侦是要销毁的，所以空间也就释放了，要返回上一层栈侦继续计算+号后面的数，所以它所需要的空间不是一直累加起来的，之后产生的栈侦空间都小于递归最深的那一次所耗费的空间。递归的深度*每次递归所需的辅助空间的个数 ，所以空间复杂度是：O(N) 注意： 1.空间复杂度相比时间复杂度分析要少。2.对于递归算法来说，代码一般都比较简短，算法本身所占用的存储空间较少，但运行时需要占用较多的临时工作单元，并且可能导致栈溢出，当需要计算的数稍大一点，就需要很长的计算时间，因此需要灵活使用递归 参考《大话数据结构》斐波那契数与二分法的递归与非递归算法及其复杂度分析]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
</search>
